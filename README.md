# Text-Summarization-Using-Ollama-and-RAG
This simple project help summarizing a text or pdf document(supported formats: .txt and .pdf). You can also automatically summarize all the documents in a folder using a powerful model from the Ollama API for free.

# ğŸ“„ AI-Powered PDF Summarizer

This project is a simple AI-powered tool to summarize PDF documents.
It can process an entire folder of PDF files, extract their contents, and generate concise summaries.

âœ… Handles multiple PDFs at once

âœ… Outputs summaries either in the same folder or in a new output/ folder

âœ… Supports Ollama as the AI inference engine

âœ… Defaults to the deepseek-r1:7b model if no other Ollama model is specified

# ğŸš€ Features
Extracts text from all PDFs in a given folder.

Uses Ollama API models for natural language summarization.

Saves results in .txt format alongside the original PDFs or in an output/ folder.

Works on Windows, macOS, and Linux.

# ğŸ”§ Installation Guide
1. Clone the Repository

bash
git clone https://github.com/dimitribekale/Text-Summarization-Using-LLM-and-RAG
.git
cd pdf-summarizer

2. Create a Python Virtual Environment

ğŸŸ¦ Windows (PowerShell)

powershell
python -m venv .venv
.venv\Scripts\activate

ğŸ macOS

bash
python3 -m venv .venv
source .venv/bin/activate

ğŸ§ Linux

bash
python3 -m venv .venv
source .venv/bin/activate

3. Install Dependencies

bash
pip install --upgrade pip
pip install -r requirements.txt

ğŸ§  Setting up Ollama

This project uses Ollama for AI inference.

1. Install Ollama
macOS: Download from the official website.

Linux: Follow installation instructions from Ollama docs.

Windows: Use WSL2 (Ubuntu recommended) or the native Windows Ollama installer (if available).

2. Run the Ollama Service
bash
ollama serve
This runs the Ollama server in the background.

3. Pull Your Desired Model
By default, the tool uses:

bash
ollama pull deepseek-r1:7b
If you want to use another model (e.g., llama3:8b):

bash
ollama pull llama3:8b

# â–¶ï¸ Usage

Run the script:

bash
python main.py --input-folder path/to/pdf/folder

Options:

--input-folder â†’ Path to folder containing PDFs.

--output-folder â†’ Path to store summaries. If not provided, defaults to output/.

--model â†’ Ollama model to use (default: deepseek-r1:7b).

Example:

bash
python summarize.py --input-folder ./pdfs --output-folder ./summaries --model llama3:8b
ğŸ—‚ Project Structure

text
ğŸ“‚ src/
 â”£ ğŸ“œ main.py             # Main script
 â”£ ğŸ“œ rag.py              # Retrieval component script
 â”£ ğŸ“œ read_file.py        # File reader component script
 â”£ ğŸ“œ requirements.txt    # Python dependencies
 â”£ ğŸ“œ README.md           # Project documentation
 â”£ ğŸ“‚ pdfs/               # (Example folder for input PDFs)
 â”— ğŸ“‚ output/             # Summarized results (auto-created)

âš¡ï¸ Troubleshooting
Ollama not running? â†’ Ensure you started ollama serve.

Model not found? â†’ Run ollama pull <model-name> before executing script.

PDF text not extracted correctly? â†’ Some PDFs are images; you may need OCR (planned feature).

ğŸ“Œ Roadmap

 Support for batch summarization with progress tracking

ğŸ‘¨ğŸ’» Contribution
Feel free to fork this repo, open issues, or submit PRs for improvements.

ğŸ“œ License
MIT License â€“ Free to use and modify.
